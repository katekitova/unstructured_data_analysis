{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZgN2IsNnmH-"
   },
   "source": [
    "### Обязательные библиотеки:\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — инструмент для решения различных задач NLP (тематическое моделирование, представление текстов, ...);\n",
    "- [Numpy](http://www.numpy.org) — библиотека для векторных вычислений;\n",
    "Для выполнения дополнительных заданий (по желанию)\n",
    "- [Pandas](https://pandas.pydata.org) - библиотека для анализа табличных данных;\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — библилиотека с алгоритмами классического машинного обучения;\n",
    "- [matplotlib](https://matplotlib.org) - библиотека для построения графиков;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxA-zK5OnmIA"
   },
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICOFfqI7nmIB",
    "outputId": "33d60ac0-41fc-437e-9cb9-5d6a254f5f01"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Константы\n",
    "DATA_DIR = 'data'\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5gJDANXnmIB"
   },
   "source": [
    "### Модель, генерирующая вектора слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/anaconda3/envs/VecWordRepresentation/lib/python3.10/site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/VecWordRepresentation/lib/python3.10/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/VecWordRepresentation/lib/python3.10/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/VecWordRepresentation/lib/python3.10/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/VecWordRepresentation/lib/python3.10/site-packages (from requests) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gAkkfYQsnmIC",
    "outputId": "8683d4d7-6030-4124-d55b-87fa9af7b3a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Скачиваем архив из http://vectors.nlpl.eu/repository/20/213.zip...\n",
      "Архив успешно скачан\n",
      "Распаковываем архив...\n",
      "Архив успешно распакован\n"
     ]
    }
   ],
   "source": [
    "from utils import download_model\n",
    "\n",
    "download_model(data_dir=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HYFLEQo-nmIC"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "wv_embeddings = gensim.models.fasttext.FastTextKeyedVectors.load(\n",
    "    DATA_DIR + \"/model/model.model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "а) из векторного пространства модели\n",
    "\n",
    "Берётся целевое слово и для него извлекается вектор затем выбирается большое число ближайших кандидатов (topn=500)\n",
    "\n",
    "Кандидаты фильтруются по порогу косинусной близости sim_threshold=0.48, исключаются словоформы целевого слова — для этого сравнивается одинаковый префикс, также с помощью регулярного выражения убираются токены со знаками препинания и цифрами внутри\n",
    "\n",
    "Результат функции - список пар (слово, косинусная близость) для слов из всего словаря модели, которые наиболее близки к целевому слову"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "def get_similar_from_model(\n",
    "    target: str,\n",
    "    wv_embeddings,\n",
    "    topn: int = 500,\n",
    "    sim_threshold: float = 0.4,\n",
    "    prefix_len: int = 3,\n",
    "    regex_word: str = r\"^[А-Яа-яЁёA-Za-z-]+$\"\n",
    ") -> List[Tuple[str, float]]:\n",
    "    raw = wv_embeddings.most_similar(target, topn=topn)\n",
    "    pref = target[:prefix_len].lower()\n",
    "    result = []\n",
    "    for w, sim in raw:\n",
    "        if sim < sim_threshold:\n",
    "            continue\n",
    "        if w.lower().startswith(pref):# отсев словоформ\n",
    "            continue\n",
    "        if not re.match(regex_word, w):# отсев странных токенов\n",
    "            continue\n",
    "        result.append((w, sim))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('калифорния', 0.7500761151313782),\n",
       " ('пенсильвания', 0.7051037549972534),\n",
       " ('америка', 0.690152645111084),\n",
       " ('орегон', 0.6841040849685669),\n",
       " ('техас', 0.6772359609603882),\n",
       " ('флорида', 0.66839998960495),\n",
       " ('канада', 0.6636497974395752),\n",
       " ('сша', 0.6608284711837769),\n",
       " ('иллинойс', 0.660652756690979),\n",
       " ('миннесота', 0.6557710766792297),\n",
       " ('айова', 0.6554882526397705),\n",
       " ('аризона', 0.6543841361999512),\n",
       " ('алабама', 0.6538816094398499),\n",
       " ('теннесси', 0.6483874917030334),\n",
       " ('нь', 0.644929826259613),\n",
       " ('мичиган', 0.6390215754508972),\n",
       " ('висконсин', 0.6380236148834229),\n",
       " ('каролина', 0.6333165764808655),\n",
       " ('невада', 0.6325955986976624),\n",
       " ('коннектикут', 0.6264144778251648)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_word = \"штат\"\n",
    "similar_model = get_similar_from_model(target_word, wv_embeddings,topn=500, sim_threshold=0.48)\n",
    "similar_model[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "б)из списка слов, содержащихся в некотором файле\n",
    "\n",
    "Получаю вектор целевого слова many_meaning_word = wv_embeddings[<слово>]. Читаю файл и разбиваю его на токены. Для каждого слова из файла, которое присутствует в словаре модели, вычисляю косинусную близость между его вектором и вектором слова по sklearn.metrics.pairwise.cosine_similarity\n",
    "\n",
    "Формирую троеку (слово, вектор, близость) и сортирую по убыванию близости. С помощью цикла ищу пока значение похожести не опускается ниже заданного порога также 0.48. Беру только верхнюю часть списка, получаю список слов и соответствующие им эмбеддинги. В конце нормализую эмбеддинги найденных слов по длине"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_similar_from_file(\n",
    "    target: str,\n",
    "    wv_embeddings,\n",
    "    filepath: str = \"words_big.txt\",\n",
    "    sim_threshold: float = 0.48,\n",
    "):\n",
    "    many_meaning_word = wv_embeddings[target]\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as reader:\n",
    "        words = reader.read().split()[1:]\n",
    "\n",
    "    words_with_similarity = []\n",
    "    for word in words:\n",
    "        if word not in wv_embeddings:\n",
    "            continue\n",
    "        sim = cosine_similarity(many_meaning_word.reshape(1, -1),wv_embeddings[word].reshape(1, -1))[0, 0]\n",
    "        words_with_similarity.append((word, wv_embeddings[word], sim))\n",
    "\n",
    "    words_with_similarity = sorted(words_with_similarity,key=lambda pair: pair[2],reverse=True)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(words_with_similarity) and words_with_similarity[i][2] >= sim_threshold:\n",
    "        i += 1\n",
    "    split_line = i \n",
    "\n",
    "    words_splitted = words_with_similarity[:split_line]\n",
    "    words_res = []\n",
    "    embeddings = []\n",
    "    for (word, emb, _) in words_splitted:\n",
    "        words_res.append(word)\n",
    "        embeddings.append(emb)\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    return words_res, embeddings, words_splitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нашла какой-то случайный словарь русских слов на github: https://github.com/danakt/russian-words/blob/master/russian.txt \n",
    "\n",
    "Сохраним его как words_big.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/danakt/russian-words/master/russian.txt\"\n",
    "resp = requests.get(url)\n",
    "resp.raise_for_status()\n",
    "with open(\"words_big.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Долго, но зато много слов и работает :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено слов: 740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['штат',\n",
       " 'калифорния',\n",
       " 'штаты',\n",
       " 'америка',\n",
       " 'Калифорния',\n",
       " 'нь',\n",
       " 'каролина',\n",
       " 'арканзас',\n",
       " 'Миннесота',\n",
       " 'юта',\n",
       " 'Вайоминг',\n",
       " 'Арканзас',\n",
       " 'пенсильванцем',\n",
       " 'Висконсин',\n",
       " 'миннесотский',\n",
       " 'Джорджия',\n",
       " 'Висконсина',\n",
       " 'чикаго',\n",
       " 'Филадельфия',\n",
       " 'Вайоминга']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_word = \"штат\"\n",
    "words_file, emb_file, raw_file = get_similar_from_file(target_word,wv_embeddings,filepath=\"words_big.txt\",sim_threshold=0.48)\n",
    "print(\"Найдено слов:\", len(words_file))\n",
    "words_file[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение а) и б)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего слов в списке a): 294\n",
      "Всего слов в списке б): 740\n",
      "Пересечение списков: 79\n"
     ]
    }
   ],
   "source": [
    "set_model = {w for w, _ in similar_model}\n",
    "set_file  = set(words_file)\n",
    "\n",
    "both = set_model & set_file\n",
    "only_a = set_model - set_file\n",
    "only_b = set_file - set_model\n",
    "\n",
    "print(\"Всего слов в списке a):\", len(set_model))\n",
    "print(\"Всего слов в списке б):\", len(set_file))\n",
    "print(\"Пересечение списков:\", len(both))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры общих слов (пересечение):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['иммигрант',\n",
       " 'бостон',\n",
       " 'столица',\n",
       " 'графство',\n",
       " 'колледж',\n",
       " 'англия',\n",
       " 'университет',\n",
       " 'россия',\n",
       " 'кантон',\n",
       " 'нь',\n",
       " 'стажер',\n",
       " 'городок',\n",
       " 'британия',\n",
       " 'онтарийский',\n",
       " 'калифорния',\n",
       " 'бирмингемский',\n",
       " 'мельбурнский',\n",
       " 'район',\n",
       " 'иммигрировать',\n",
       " 'конгресс']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(both)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры слов только из a):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['рочестер',\n",
       " 'санта-фе',\n",
       " 'высококвалифицировать',\n",
       " 'анкоридж',\n",
       " 'филадельфия',\n",
       " 'санта-крус',\n",
       " 'абердина',\n",
       " 'керала',\n",
       " 'техас',\n",
       " 'питсбург',\n",
       " 'огайо',\n",
       " 'бруклин',\n",
       " 'пуэрто-рико',\n",
       " 'принстон',\n",
       " 'ньюпорт',\n",
       " 'колорадо',\n",
       " 'канзася',\n",
       " 'дублин',\n",
       " 'оклахом',\n",
       " 'коннектикута']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(only_a)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры слов только из б):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Кингстон',\n",
       " 'Висконсин',\n",
       " 'канберрское',\n",
       " 'пенсильванские',\n",
       " 'алабамского',\n",
       " 'атлантам',\n",
       " 'калифорниевом',\n",
       " 'Луизиана',\n",
       " 'западно',\n",
       " 'найробийский',\n",
       " 'миннесотских',\n",
       " 'Канада',\n",
       " 'висконсинском',\n",
       " 'мельбурнском',\n",
       " 'ассамский',\n",
       " 'теннесийских',\n",
       " 'миссисипская',\n",
       " 'иммиграционном',\n",
       " 'конгрессмены',\n",
       " 'миссисипской']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(only_b)[:20]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "VecWordRepresentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
